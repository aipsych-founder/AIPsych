// Simple performance test
import fetch from 'node-fetch';

const API_BASE = 'http://localhost:3001';

async function test() {
  console.log('üöÄ Testing API Latencies\n');
  
  // Test 1: Chat API
  console.log('1Ô∏è‚É£  Chat API Test');
  for (let i = 1; i <= 2; i++) {
    const start = Date.now();
    const res = await fetch(`${API_BASE}/api/psych`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ message: 'Hello, how are you?' })
    });
    const data = await res.json();
    const elapsed = Date.now() - start;
    console.log(`   Request ${i}: ${elapsed}ms total (server: ${data.durationMs}ms)`);
  }
  
  // Test 2: TTS
  console.log('\n2Ô∏è‚É£  TTS Streaming Test');
  for (let i = 1; i <= 2; i++) {
    const start = Date.now();
    const res = await fetch(`${API_BASE}/api/tts`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ text: 'Good morning, how are you feeling today?' })
    });
    const buffer = await res.arrayBuffer();
    const elapsed = Date.now() - start;
    console.log(`   Request ${i}: ${elapsed}ms (${buffer.byteLength} bytes)`);
  }
  
  console.log('\n‚úÖ Tests completed!\n');
  console.log('üìä Expected latencies with optimizations:');
  console.log('   - Chat API: 1500-2500ms (OpenAI latency)');
  console.log('   - TTS: 2500-4000ms (OpenAI latency)');
  console.log('   - With streaming, audio starts playing immediately');
  console.log('   - Compression reduces transfer size by ~70%');
}

test().catch(console.error).finally(() => process.exit(0));
